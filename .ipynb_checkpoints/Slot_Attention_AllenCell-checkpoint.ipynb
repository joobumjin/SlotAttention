{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iTGnEyGT7m0s"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "import tifffile\n",
    "import quilt3 as q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aics/actk',\n",
       " 'aics/aics_mnist',\n",
       " 'aics/cell_line_exomes',\n",
       " 'aics/cell_line_rnaseq',\n",
       " 'aics/data_handoff_4dn',\n",
       " 'aics/hipsc_12x_overview_image_dataset',\n",
       " 'aics/hipsc_single_cell_image_dataset',\n",
       " 'aics/hipsc_single_cell_image_dataset_supp_myh10',\n",
       " 'aics/hipsc_single_edge_cell_image_dataset',\n",
       " 'aics/hipsc_single_i1_cell_image_dataset',\n",
       " 'aics/hipsc_single_i2_cell_image_dataset',\n",
       " 'aics/hipsc_single_m1_cell_image_dataset',\n",
       " 'aics/hipsc_single_m2_cell_image_dataset',\n",
       " 'aics/hipsc_single_nonedge_cell_image_dataset',\n",
       " 'aics/integrated_transcriptomics_structural_organization_hipsc_cm',\n",
       " 'aics/label-free-imaging-collection',\n",
       " 'aics/laminb1_sample_data',\n",
       " 'aics/mitotic_annotation',\n",
       " 'aics/nuclear_project_dataset_1',\n",
       " 'aics/nuclear_project_dataset_2',\n",
       " 'aics/nuclear_project_dataset_3',\n",
       " 'aics/nuclear_project_dataset_4',\n",
       " 'aics/pipeline_integrated_cell',\n",
       " 'aics/pipeline_integrated_single_cell',\n",
       " 'aics/segmenter_model_zoo',\n",
       " 'aics/wtc11_hipsc_cardiomyocyte_scrnaseq_d0_to_d90',\n",
       " 'aics/wtc11_linkedread_wgs',\n",
       " 'aics/wtc11_short_read_genome_sequence',\n",
       " 'test/tmp']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(q3.list_packages(\"s3://allencell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading manifest: 100%|████████████████| 16.4M/16.4M [00:03<00:00, 4.87MB/s]\n",
      "Loading manifest: 100%|█████████████████████| 49342/49342 [00:00<00:00, 95.3k/s]\n"
     ]
    }
   ],
   "source": [
    "label_free = q3.Package.browse(\n",
    "    \"aics/label-free-imaging-collection\",\n",
    "    registry=\"s3://allencell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imread_bytes = lambda b: tifffile.TiffFile(io.BytesIO(b)).asarray()\n",
    "img = label_free[\"cells_2d\"][\"fov-0_CellIndex-12.tiff\"](imread_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1, 158, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "/var/folders/c8/hr1zscmn3tz4k0q2tr2xvbmc0000gn/T/ipykernel_63627/1901004211.py:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "  plt.imshow(np.transpose(img, (1, 0, 2, 3)[0][0]))\n",
      "/var/folders/c8/hr1zscmn3tz4k0q2tr2xvbmc0000gn/T/ipykernel_63627/1901004211.py:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "  plt.imshow(np.transpose(img, (1, 0, 2, 3)[0][0]))\n",
      "/var/folders/c8/hr1zscmn3tz4k0q2tr2xvbmc0000gn/T/ipykernel_63627/1901004211.py:1: SyntaxWarning: 'int' object is not subscriptable; perhaps you missed a comma?\n",
      "  plt.imshow(np.transpose(img, (1, 0, 2, 3)[0][0]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mtranspose(img, \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.transpose(img, (1, 0, 2, 3))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgnYaJ67cjgW"
   },
   "outputs": [],
   "source": [
    "def preprocess_clevr(features, resolution, apply_crop=False,\n",
    "                     get_properties=True, max_n_objects=10):\n",
    "  \"\"\"Preprocess CLEVR.\"\"\"\n",
    "  image = tf.cast(features[\"image\"], dtype=tf.float32)\n",
    "  image = ((image / 255.0) - 0.5) * 2.0  # Rescale to [-1, 1].\n",
    "\n",
    "  image = tf.image.resize(\n",
    "      image, resolution, method=tf.image.ResizeMethod.BILINEAR)\n",
    "  image = tf.clip_by_value(image, -1., 1.)\n",
    "    \n",
    "  features = {\"image\": image}\n",
    "  return features\n",
    "\n",
    "\n",
    "def build_clevr(split, resolution=(128, 128), shuffle=False, max_n_objects=10,\n",
    "                num_eval_examples=512, apply_crop=False):\n",
    "  \"\"\"Build CLEVR dataset.\"\"\"\n",
    "  if split == \"train\" or split == \"train_eval\":\n",
    "    ds = tfds.load(\"clevr:3.1.0\", split=\"train\", shuffle_files=shuffle)\n",
    "    if split == \"train\":\n",
    "      ds = ds.skip(num_eval_examples)\n",
    "    elif split == \"train_eval\":\n",
    "      # Instead of taking the official validation split, we take a smaller split\n",
    "      # from the training dataset to monitor AP scores during training.\n",
    "      ds = ds.take(num_eval_examples)\n",
    "  else:\n",
    "    ds = tfds.load(\"clevr:3.1.0\", split=split, shuffle_files=shuffle)\n",
    "\n",
    "  def _preprocess_fn(x, resolution, max_n_objects=max_n_objects):\n",
    "    return preprocess_clevr(\n",
    "        x, resolution, apply_crop=apply_crop, get_properties=get_properties,\n",
    "        max_n_objects=max_n_objects)\n",
    "  ds = ds.map(lambda x: _preprocess_fn(x, resolution))\n",
    "  return ds\n",
    "\n",
    "\n",
    "def build_clevr_iterator(batch_size, split, **kwargs):\n",
    "  ds = build_clevr(split=split, **kwargs)\n",
    "  ds = ds.repeat(-1)\n",
    "  ds = ds.batch(batch_size, drop_remainder=True)\n",
    "  return iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fTdux-_354b7"
   },
   "outputs": [],
   "source": [
    "\"\"\"Slot Attention model for object discovery and set prediction.\"\"\"\n",
    "\n",
    "class SlotAttention(layers.Layer):\n",
    "  \"\"\"Slot Attention module.\"\"\"\n",
    "\n",
    "  def __init__(self, num_iterations, num_slots, slot_size, mlp_hidden_size,\n",
    "               epsilon=1e-8):\n",
    "    \"\"\"Builds the Slot Attention module.\n",
    "    Args:\n",
    "      num_iterations: Number of iterations.\n",
    "      num_slots: Number of slots.\n",
    "      slot_size: Dimensionality of slot feature vectors.\n",
    "      mlp_hidden_size: Hidden layer size of MLP.\n",
    "      epsilon: Offset for attention coefficients before normalization.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.num_iterations = num_iterations\n",
    "    self.num_slots = num_slots\n",
    "    self.slot_size = slot_size\n",
    "    self.mlp_hidden_size = mlp_hidden_size\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    self.norm_inputs = layers.LayerNormalization()\n",
    "    self.norm_slots = layers.LayerNormalization()\n",
    "    self.norm_mlp = layers.LayerNormalization()\n",
    "\n",
    "    # Parameters for Gaussian init (shared by all slots).   # Intialize slots randomly at first \n",
    "    self.slots_mu = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],   # slot_size: Dimensionality of slot feature vectors.\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_mu\")\n",
    "    self.slots_log_sigma = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_log_sigma\")\n",
    "\n",
    "    # Linear maps for the attention module.\n",
    "    self.project_q = layers.Dense(self.slot_size, use_bias=False, name=\"q\")\n",
    "    self.project_k = layers.Dense(self.slot_size, use_bias=False, name=\"k\")\n",
    "    self.project_v = layers.Dense(self.slot_size, use_bias=False, name=\"v\")\n",
    "\n",
    "    # Slot update functions.\n",
    "    self.gru = layers.GRUCell(self.slot_size)\n",
    "    self.mlp = tf.keras.Sequential([\n",
    "        layers.Dense(self.mlp_hidden_size, activation=\"relu\"),\n",
    "        layers.Dense(self.slot_size)\n",
    "    ], name=\"mlp\")\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # `inputs` has shape [batch_size, num_inputs, inputs_size].\n",
    "    inputs = self.norm_inputs(inputs)  # Apply layer norm to the input.\n",
    "    k = self.project_k(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create key vectors (based on inputs)\n",
    "    v = self.project_v(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create value vectors (based on inputs)\n",
    "\n",
    "    # Initialize the slots. Shape: [batch_size, num_slots, slot_size].\n",
    "    slots = self.slots_mu + tf.exp(self.slots_log_sigma) * tf.random.normal(\n",
    "        [tf.shape(inputs)[0], self.num_slots, self.slot_size])  # size: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    # Multiple rounds of attention.\n",
    "    for _ in range(self.num_iterations):\n",
    "      slots_prev = slots\n",
    "      slots = self.norm_slots(slots)\n",
    "\n",
    "      # Attention.\n",
    "      q = self.project_q(slots)  # Shape: [batch_size, num_slots, slot_size].  # create query vectors (based on slots)\n",
    "      q *= self.slot_size ** -0.5  # Normalization.\n",
    "      attn_logits = tf.keras.backend.batch_dot(k, q, axes=-1) # Batchwise dot product.\n",
    "      attn = tf.nn.softmax(attn_logits, axis=-1)\n",
    "      # `attn` has shape: [batch_size, num_inputs, num_slots]. \n",
    "      # attn represents how much attention each slot should pay to the features \n",
    "\n",
    "      # Weigted mean.\n",
    "      attn += self.epsilon\n",
    "      attn /= tf.reduce_sum(attn, axis=-2, keepdims=True) # summation; sum across the batch_size \n",
    "      updates = tf.keras.backend.batch_dot(attn, v, axes=-2)\n",
    "      # `updates` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "      # Slot update.\n",
    "      slots, _ = self.gru(updates, [slots_prev])   # output after gru has shape: [batch_size, num_slots, slot_size]\n",
    "      slots += self.mlp(self.norm_mlp(slots))      # # output after mlp has shape: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    return slots\n",
    "\n",
    "\n",
    "def spatial_broadcast(slots, resolution):\n",
    "  \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n",
    "  # `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "  slots = tf.reshape(slots, [-1, slots.shape[-1]])[:, None, None, :]\n",
    "  grid = tf.tile(slots, [1, resolution[0], resolution[1], 1])   # this operation creates a new tensor by replicating input multiples times\n",
    "  # `grid` has shape: [batch_size*num_slots, width, height, slot_size].\n",
    "  return grid\n",
    "\n",
    "\n",
    "def spatial_flatten(x):\n",
    "  return tf.reshape(x, [-1, x.shape[1] * x.shape[2], x.shape[-1]])\n",
    "\n",
    "\n",
    "def unstack_and_split(x, batch_size, num_channels=3):\n",
    "  \"\"\"Unstack batch dimension and split into channels and alpha mask.\"\"\"\n",
    "  unstacked = tf.reshape(x, [batch_size, -1] + x.shape.as_list()[1:])\n",
    "  channels, masks = tf.split(unstacked, [num_channels, 1], axis=-1)\n",
    "  return channels, masks\n",
    "\n",
    "\n",
    "class SlotAttentionAutoEncoder(layers.Layer):\n",
    "  \"\"\"Slot Attention-based auto-encoder for object discovery.\"\"\"\n",
    "\n",
    "  def __init__(self, resolution, num_slots, num_iterations):\n",
    "    \"\"\"Builds the Slot Attention-based auto-encoder.\n",
    "    Args:\n",
    "      resolution: Tuple of integers specifying width and height of input image.\n",
    "      num_slots: Number of slots in Slot Attention.\n",
    "      num_iterations: Number of iterations in Slot Attention.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.resolution = resolution\n",
    "    self.num_slots = num_slots\n",
    "    self.num_iterations = num_iterations\n",
    "\n",
    "    self.encoder_cnn = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
    "        # Can be a single integer to specify the same value for all spatial dimensions.\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\")\n",
    "    ], name=\"encoder_cnn\")\n",
    "\n",
    "    self.decoder_initial_size = (8, 8)\n",
    "    self.decoder_cnn = tf.keras.Sequential([\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),  # filters = 64 (number of output channels); kernel_size = 5 (specify the height and width of the 2D convolution window)\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(1, 1), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            4, 3, strides=(1, 1), padding=\"SAME\", activation=None)\n",
    "    ], name=\"decoder_cnn\")\n",
    "\n",
    "    self.encoder_pos = SoftPositionEmbed(64, self.resolution)\n",
    "    self.decoder_pos = SoftPositionEmbed(64, self.decoder_initial_size)\n",
    "\n",
    "    self.layer_norm = layers.LayerNormalization()\n",
    "    self.mlp = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64)\n",
    "    ], name=\"feedforward\")\n",
    "\n",
    "    self.slot_attention = SlotAttention(\n",
    "        num_iterations=self.num_iterations,\n",
    "        num_slots=self.num_slots,\n",
    "        slot_size=64,\n",
    "        mlp_hidden_size=128)\n",
    "\n",
    "  def call(self, image):\n",
    "    # `image` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "    # Convolutional encoder with position embedding.\n",
    "    x = self.encoder_cnn(image)  # CNN Backbone.\n",
    "    x = self.encoder_pos(x)  # Position embedding.\n",
    "    x = spatial_flatten(x)  # Flatten spatial dimensions (treat image as set).\n",
    "    x = self.mlp(self.layer_norm(x))  # Feedforward network on set.\n",
    "    # `x` has shape: [batch_size, width*height, input_size(64)].\n",
    "\n",
    "    # Slot Attention module.\n",
    "    slots = self.slot_attention(x)\n",
    "    # `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "    # Spatial broadcast decoder.\n",
    "    x = spatial_broadcast(slots, self.decoder_initial_size)\n",
    "    # `x` has shape: [batch_size*num_slots, width_init, height_init, slot_size].\n",
    "    x = self.decoder_pos(x)\n",
    "    x = self.decoder_cnn(x)\n",
    "    # `x` has shape: [batch_size*num_slots, width, height, num_channels+1].\n",
    "\n",
    "    # Undo combination of slot and batch dimension; split alpha masks.\n",
    "    recons, masks = unstack_and_split(x, batch_size=image.shape[0])\n",
    "    # `recons` has shape: [batch_size, num_slots, width, height, num_channels].\n",
    "    # `masks` has shape: [batch_size, num_slots, width, height, 1].\n",
    "\n",
    "    # Normalize alpha masks over slots.\n",
    "    masks = tf.nn.softmax(masks, axis=1)\n",
    "    recon_combined = tf.reduce_sum(recons * masks, axis=1)  # Recombine image.\n",
    "    # `recon_combined` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "    return recon_combined, recons, masks, slots\n",
    "    \n",
    "\n",
    "def build_grid(resolution):\n",
    "  ranges = [np.linspace(0., 1., num=res) for res in resolution]\n",
    "  grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "  grid = np.stack(grid, axis=-1)\n",
    "  grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "  grid = np.expand_dims(grid, axis=0)\n",
    "  grid = grid.astype(np.float32)\n",
    "  return np.concatenate([grid, 1.0 - grid], axis=-1)\n",
    "\n",
    "\n",
    "class SoftPositionEmbed(layers.Layer):\n",
    "  \"\"\"Adds soft positional embedding with learnable projection.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size, resolution):\n",
    "    \"\"\"Builds the soft position embedding layer.\n",
    "    Args:\n",
    "      hidden_size: Size of input feature dimension.\n",
    "      resolution: Tuple of integers specifying width and height of grid.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.dense = layers.Dense(hidden_size, use_bias=True)\n",
    "    self.grid = build_grid(resolution)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.dense(self.grid)\n",
    "\n",
    "\n",
    "def build_model(resolution, batch_size, num_slots, num_iterations,\n",
    "                num_channels=3, model_type=\"object_discovery\"):\n",
    "  \"\"\"Build keras model.\"\"\"\n",
    "  if model_type == \"object_discovery\":\n",
    "    model_def = SlotAttentionAutoEncoder\n",
    "  else:\n",
    "    raise ValueError(\"Invalid name for model type.\")\n",
    "\n",
    "  image = tf.keras.Input(list(resolution) + [num_channels], batch_size) # shape = list(resolution) + [num_channels]\n",
    "  outputs = model_def(resolution, num_slots, num_iterations)(image)  # initialize + call\n",
    "  model = tf.keras.Model(inputs=image, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5j9ZfBpeNa3",
    "outputId": "baae4701-4c3e-4700-b008-f2532dbdee0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training loop for object discovery with Slot Attention.\"\"\"\n",
    "\n",
    "# We use `tf.function` compilation to speed up execution. For debugging,\n",
    "# consider commenting out the `@tf.function` decorator.\n",
    "\n",
    "\n",
    "def l2_loss(prediction, target):\n",
    "  return tf.reduce_mean(tf.math.squared_difference(prediction, target))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch, model, optimizer):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  # Get the prediction of the models and compute the loss.\n",
    "  with tf.GradientTape() as tape:\n",
    "    preds = model(batch[\"image\"], training=True)\n",
    "    recon_combined, recons, masks, slots = preds\n",
    "    loss_value = l2_loss(recon_combined, batch[\"image\"])\n",
    "    del recons, masks, slots  # Unused.\n",
    "\n",
    "  # Get and apply gradients.\n",
    "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))   \n",
    "\n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses): \n",
    "    \"\"\"\n",
    "    Uses Matplotlib to visualize the losses of our model.\n",
    "    :param losses: list of loss data stored from train. Can use the model's loss_list \n",
    "    field \n",
    "\n",
    "    NOTE: DO NOT EDIT\n",
    "\n",
    "    :return: doesn't return anything, a plot should pop-up \n",
    "    \"\"\"\n",
    "    x = [i for i in range(len(losses))]\n",
    "    plt.plot(x, losses)\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 15:47:29.353231: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-16 15:47:29.353342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 15:47:29.580398: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "Training Epochs:   0%|                                 | 0/5000 [00:00<?, ?it/s]2022-09-16 15:47:31.000186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "Training Epochs:   0%|                     | 24/5000 [03:07<11:04:06,  8.01s/it]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model.\n",
    "batch_size = 64\n",
    "num_slots = 7\n",
    "num_iterations = 3\n",
    "base_learning_rate = 0.0004\n",
    "num_train_steps = 5000\n",
    "warmup_steps = 5\n",
    "decay_rate = 0.5\n",
    "decay_steps = 100000\n",
    "tf.random.set_seed(0)\n",
    "resolution = (128, 128)\n",
    "\n",
    "# Build dataset iterators, optimizers and model.\n",
    "data_iterator = build_clevr_iterator(\n",
    "    batch_size, split=\"train\", resolution=resolution, shuffle=True,\n",
    "    max_n_objects=6, get_properties=False, apply_crop=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(base_learning_rate, epsilon=1e-08)\n",
    "\n",
    "model = build_model(resolution, batch_size, num_slots,\n",
    "                    num_iterations, model_type=\"object_discovery\")\n",
    "  \n",
    "# Prepare checkpoint manager.\n",
    "global_step = tf.Variable(\n",
    "    0, trainable=False, name=\"global_step\", dtype=tf.int64)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for _ in tqdm(range(num_train_steps), desc='Training Epochs'):\n",
    "    batch = next(data_iterator)\n",
    "\n",
    "    # Learning rate warm-up.\n",
    "    if global_step < warmup_steps:\n",
    "      learning_rate = base_learning_rate * tf.cast(\n",
    "          global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)\n",
    "    else:\n",
    "      learning_rate = base_learning_rate\n",
    "    \n",
    "    learning_rate = learning_rate * (decay_rate ** (\n",
    "        tf.cast(global_step, tf.float32) / tf.cast(decay_steps, tf.float32)))\n",
    "    optimizer.lr = learning_rate.numpy()\n",
    "\n",
    "    loss_value = train_step(batch, model, optimizer)\n",
    "    losses.append(loss_value)\n",
    "\n",
    "    # Update the global step. We update it before logging the loss and saving\n",
    "    # the model so that the last checkpoint is saved at the last iteration.\n",
    "    global_step.assign_add(1)\n",
    "    \n",
    "visualize_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1PWP4Q77v9k"
   },
   "outputs": [],
   "source": [
    "def renormalize(x):\n",
    "  \"\"\"Renormalize from [-1, 1] to [0, 1].\"\"\"\n",
    "  return x / 2. + 0.5\n",
    "\n",
    "def get_prediction(model, batch, idx=0):\n",
    "  recon_combined, recons, masks, slots = model(batch[\"image\"])\n",
    "  image = renormalize(batch[\"image\"])[idx]\n",
    "  recon_combined = renormalize(recon_combined)[idx]\n",
    "  recons = renormalize(recons)[idx]\n",
    "  masks = masks[idx]\n",
    "  return image, recon_combined, recons, masks, slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bhXod7Q7xXB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "resolution = (128,128)\n",
    "data_iterator = build_clevr_iterator(\n",
    "    batch_size, split=\"validation\", resolution=resolution, shuffle=True,\n",
    "    max_n_objects=6, get_properties=False, apply_crop=True)\n",
    "\n",
    "batch = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "plt.imshow(renormalize(batch[\"image\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzJouS6a7yvZ"
   },
   "outputs": [],
   "source": [
    "image, recon_combined, recons, masks, slots = get_prediction(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "num_slots = len(masks)\n",
    "fig, ax = plt.subplots(1, num_slots + 2, figsize=(15, 2))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(recon_combined)\n",
    "ax[1].set_title('Recon.')\n",
    "for i in range(num_slots):\n",
    "  ax[i + 2].imshow(recons[i] * masks[i] + (1 - masks[i]))\n",
    "  ax[i + 2].set_title('Slot %s' % str(i + 1))\n",
    "for i in range(len(ax)):\n",
    "  ax[i].grid(False)\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recon_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Slot Attention.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
