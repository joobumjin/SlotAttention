{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iTGnEyGT7m0s"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fTdux-_354b7"
   },
   "outputs": [],
   "source": [
    "\"\"\"Slot Attention model for object discovery and set prediction.\"\"\"\n",
    "\n",
    "class SlotAttention(layers.Layer):\n",
    "  \"\"\"Slot Attention module.\"\"\"\n",
    "\n",
    "  def __init__(self, num_iterations, num_slots, slot_size, mlp_hidden_size,\n",
    "               epsilon=1e-8):\n",
    "    \"\"\"Builds the Slot Attention module.\n",
    "    Args:\n",
    "      num_iterations: Number of iterations.\n",
    "      num_slots: Number of slots.\n",
    "      slot_size: Dimensionality of slot feature vectors.\n",
    "      mlp_hidden_size: Hidden layer size of MLP.\n",
    "      epsilon: Offset for attention coefficients before normalization.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.num_iterations = num_iterations\n",
    "    self.num_slots = num_slots\n",
    "    self.slot_size = slot_size\n",
    "    self.mlp_hidden_size = mlp_hidden_size\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    self.norm_inputs = layers.LayerNormalization()\n",
    "    self.norm_slots = layers.LayerNormalization()\n",
    "    self.norm_mlp = layers.LayerNormalization()\n",
    "\n",
    "    # Parameters for Gaussian init (shared by all slots).   # Intialize slots randomly at first \n",
    "    self.slots_mu = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],   # slot_size: Dimensionality of slot feature vectors.\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_mu\")\n",
    "    self.slots_log_sigma = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_log_sigma\")\n",
    "\n",
    "    # Linear maps for the attention module.\n",
    "    self.project_q = layers.Dense(self.slot_size, use_bias=False, name=\"q\")\n",
    "    self.project_k = layers.Dense(self.slot_size, use_bias=False, name=\"k\")\n",
    "    self.project_v = layers.Dense(self.slot_size, use_bias=False, name=\"v\")\n",
    "\n",
    "    # Slot update functions.\n",
    "    self.gru = layers.GRUCell(self.slot_size)\n",
    "    self.mlp = tf.keras.Sequential([\n",
    "        layers.Dense(self.mlp_hidden_size, activation=\"relu\"),\n",
    "        layers.Dense(self.slot_size)\n",
    "    ], name=\"mlp\")\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # `inputs` has shape [batch_size, num_inputs, inputs_size].\n",
    "    inputs = self.norm_inputs(inputs)  # Apply layer norm to the input.\n",
    "    k = self.project_k(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create key vectors (based on inputs)\n",
    "    v = self.project_v(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create value vectors (based on inputs)\n",
    "\n",
    "    # Initialize the slots. Shape: [batch_size, num_slots, slot_size].\n",
    "    slots = self.slots_mu + tf.exp(self.slots_log_sigma) * tf.random.normal(\n",
    "        [tf.shape(inputs)[0], self.num_slots, self.slot_size])  # size: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    # Multiple rounds of attention.\n",
    "    for _ in range(self.num_iterations):\n",
    "      slots_prev = slots\n",
    "      slots = self.norm_slots(slots)\n",
    "\n",
    "      # Attention.\n",
    "      q = self.project_q(slots)  # Shape: [batch_size, num_slots, slot_size].  # create query vectors (based on slots)\n",
    "      q *= self.slot_size ** -0.5  # Normalization.\n",
    "      attn_logits = tf.keras.backend.batch_dot(k, q, axes=-1) # Batchwise dot product.\n",
    "      attn = tf.nn.softmax(attn_logits, axis=-1)\n",
    "      # `attn` has shape: [batch_size, num_inputs, num_slots]. \n",
    "      # attn represents how much attention each slot should pay to the features \n",
    "\n",
    "      # Weigted mean.\n",
    "      attn += self.epsilon\n",
    "      attn /= tf.reduce_sum(attn, axis=-2, keepdims=True) # summation; sum across the batch_size \n",
    "      updates = tf.keras.backend.batch_dot(attn, v, axes=-2)\n",
    "      # `updates` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "      # Slot update.\n",
    "      slots, _ = self.gru(updates, [slots_prev])   # output after gru has shape: [batch_size, num_slots, slot_size]\n",
    "      slots += self.mlp(self.norm_mlp(slots))      # # output after mlp has shape: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    return slots\n",
    "\n",
    "\n",
    "def spatial_broadcast(slots, resolution):\n",
    "  \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n",
    "  # `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "  slots = tf.reshape(slots, [-1, slots.shape[-1]])[:, None, None, :]\n",
    "  grid = tf.tile(slots, [1, resolution[0], resolution[1], 1])   # this operation creates a new tensor by replicating input multiples times\n",
    "  # `grid` has shape: [batch_size*num_slots, width, height, slot_size].\n",
    "  return grid\n",
    "\n",
    "\n",
    "def spatial_flatten(x):\n",
    "  return tf.reshape(x, [-1, x.shape[1] * x.shape[2], x.shape[-1]])\n",
    "\n",
    "\n",
    "def unstack_and_split(x, batch_size, num_channels=3):\n",
    "  \"\"\"Unstack batch dimension and split into channels and alpha mask.\"\"\"\n",
    "  unstacked = tf.reshape(x, [batch_size, -1] + x.shape.as_list()[1:])\n",
    "  channels, masks = tf.split(unstacked, [num_channels, 1], axis=-1)\n",
    "  return channels, masks\n",
    "\n",
    "\n",
    "class SlotAttentionAutoEncoder(layers.Layer):\n",
    "  \"\"\"Slot Attention-based auto-encoder for object discovery.\"\"\"\n",
    "\n",
    "  def __init__(self, resolution, num_slots, num_iterations):\n",
    "    \"\"\"Builds the Slot Attention-based auto-encoder.\n",
    "    Args:\n",
    "      resolution: Tuple of integers specifying width and height of input image.\n",
    "      num_slots: Number of slots in Slot Attention.\n",
    "      num_iterations: Number of iterations in Slot Attention.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.resolution = resolution\n",
    "    self.num_slots = num_slots\n",
    "    self.num_iterations = num_iterations\n",
    "\n",
    "    self.encoder_cnn = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
    "        # Can be a single integer to specify the same value for all spatial dimensions.\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\")\n",
    "    ], name=\"encoder_cnn\")\n",
    "\n",
    "    self.decoder_initial_size = (8, 8)\n",
    "    self.decoder_cnn = tf.keras.Sequential([\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),  # filters = 64 (number of output channels); kernel_size = 5 (specify the height and width of the 2D convolution window)\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            64, 5, strides=(1, 1), padding=\"SAME\", activation=\"relu\"),\n",
    "        layers.Conv2DTranspose(\n",
    "            4, 3, strides=(1, 1), padding=\"SAME\", activation=None)\n",
    "    ], name=\"decoder_cnn\")\n",
    "\n",
    "    self.encoder_pos = SoftPositionEmbed(64, self.resolution)\n",
    "    self.decoder_pos = SoftPositionEmbed(64, self.decoder_initial_size)\n",
    "\n",
    "    self.layer_norm = layers.LayerNormalization()\n",
    "    self.mlp = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64)\n",
    "    ], name=\"feedforward\")\n",
    "\n",
    "    self.slot_attention = SlotAttention(\n",
    "        num_iterations=self.num_iterations,\n",
    "        num_slots=self.num_slots,\n",
    "        slot_size=64,\n",
    "        mlp_hidden_size=128)\n",
    "\n",
    "  def call(self, image):\n",
    "    # `image` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "    # Convolutional encoder with position embedding.\n",
    "    x = self.encoder_cnn(image)  # CNN Backbone.\n",
    "    x = self.encoder_pos(x)  # Position embedding.\n",
    "    x = spatial_flatten(x)  # Flatten spatial dimensions (treat image as set).\n",
    "    x = self.mlp(self.layer_norm(x))  # Feedforward network on set.\n",
    "    # `x` has shape: [batch_size, width*height, input_size(64)].\n",
    "\n",
    "    # Slot Attention module.\n",
    "    slots = self.slot_attention(x)\n",
    "    # `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "    # Spatial broadcast decoder.\n",
    "    x = spatial_broadcast(slots, self.decoder_initial_size)\n",
    "    # `x` has shape: [batch_size*num_slots, width_init, height_init, slot_size].\n",
    "    x = self.decoder_pos(x)\n",
    "    x = self.decoder_cnn(x)\n",
    "    # `x` has shape: [batch_size*num_slots, width, height, num_channels+1].\n",
    "\n",
    "    # Undo combination of slot and batch dimension; split alpha masks.\n",
    "    recons, masks = unstack_and_split(x, batch_size=image.shape[0])\n",
    "    # `recons` has shape: [batch_size, num_slots, width, height, num_channels].\n",
    "    # `masks` has shape: [batch_size, num_slots, width, height, 1].\n",
    "\n",
    "    # Normalize alpha masks over slots.\n",
    "    masks = tf.nn.softmax(masks, axis=1)\n",
    "    recon_combined = tf.reduce_sum(recons * masks, axis=1)  # Recombine image.\n",
    "    # `recon_combined` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "    return recon_combined, recons, masks, slots\n",
    "    \n",
    "\n",
    "def build_grid(resolution):\n",
    "  ranges = [np.linspace(0., 1., num=res) for res in resolution]\n",
    "  grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "  grid = np.stack(grid, axis=-1)\n",
    "  grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "  grid = np.expand_dims(grid, axis=0)\n",
    "  grid = grid.astype(np.float32)\n",
    "  return np.concatenate([grid, 1.0 - grid], axis=-1)\n",
    "\n",
    "\n",
    "class SoftPositionEmbed(layers.Layer):\n",
    "  \"\"\"Adds soft positional embedding with learnable projection.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size, resolution):\n",
    "    \"\"\"Builds the soft position embedding layer.\n",
    "    Args:\n",
    "      hidden_size: Size of input feature dimension.\n",
    "      resolution: Tuple of integers specifying width and height of grid.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.dense = layers.Dense(hidden_size, use_bias=True)\n",
    "    self.grid = build_grid(resolution)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.dense(self.grid)\n",
    "\n",
    "\n",
    "def build_model(resolution, batch_size, num_slots, num_iterations,\n",
    "                num_channels=3, model_type=\"object_discovery\"):\n",
    "  \"\"\"Build keras model.\"\"\"\n",
    "  if model_type == \"object_discovery\":\n",
    "    model_def = SlotAttentionAutoEncoder\n",
    "  else:\n",
    "    raise ValueError(\"Invalid name for model type.\")\n",
    "\n",
    "  image = tf.keras.Input(list(resolution) + [num_channels], batch_size) # shape = list(resolution) + [num_channels]\n",
    "  outputs = model_def(resolution, num_slots, num_iterations)(image)  # initialize + call\n",
    "  model = tf.keras.Model(inputs=image, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (1024, 1024)\n",
    "num_slots = 7\n",
    "num_iterations = 3\n",
    "\n",
    "encoder_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
    "    # Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\")\n",
    "], name=\"encoder_cnn\")\n",
    "\n",
    "decoder_initial_size = (8, 8)\n",
    "decoder_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),  \n",
    "    # filters = 64 (number of output channels); kernel_size = 5 (specify the height and width of the 2D convolution window)\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(4, 5, strides=(1, 1), padding=\"SAME\", activation=None)\n",
    "], name=\"decoder_cnn\")\n",
    "\n",
    "#strides : (x_stride, y_stride, channel_stride)\n",
    "\n",
    "encoder_pos = SoftPositionEmbed(64, resolution)\n",
    "decoder_pos = SoftPositionEmbed(64, decoder_initial_size)\n",
    "\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "mlp = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64)\n",
    "], name=\"feedforward\")\n",
    "\n",
    "slot_attention = SlotAttention(num_iterations=num_iterations, num_slots=num_slots, slot_size=64, mlp_hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Slot_Attention_AutoEnconder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 1024, 1024,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " encoder_cnn (Sequential)       (None, 1024, 1024,   309056      ['input_5[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " soft_position_embed_8 (SoftPos  (None, 1024, 1024,   320        ['encoder_cnn[0][0]']            \n",
      " itionEmbed)                    64)                                                               \n",
      "                                                                                                  \n",
      " tf.reshape_12 (TFOpLambda)     (None, 1048576, 64)  0           ['soft_position_embed_8[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 1048576, 64)  128        ['tf.reshape_12[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " feedforward (Sequential)       (None, 1048576, 64)  8320        ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " slot_attention_4 (SlotAttentio  (None, 7, 64)       54336       ['feedforward[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " tf.reshape_13 (TFOpLambda)     (None, 64)           0           ['slot_attention_4[0][0]']       \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 1, 1, 64)    0           ['tf.reshape_13[0][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.tile_4 (TFOpLambda)         (None, 8, 8, 64)     0           ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " soft_position_embed_9 (SoftPos  (None, 8, 8, 64)    320         ['tf.tile_4[0][0]']              \n",
      " itionEmbed)                                                                                      \n",
      "                                                                                                  \n",
      " decoder_cnn (Sequential)       (None, 1024, 1024,   723652      ['soft_position_embed_9[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " tf.reshape_14 (TFOpLambda)     (64, None, 1024, 10  0           ['decoder_cnn[0][0]']            \n",
      "                                24, 4)                                                            \n",
      "                                                                                                  \n",
      " tf.split_4 (TFOpLambda)        [(64, None, 1024, 1  0           ['tf.reshape_14[0][0]']          \n",
      "                                024, 3),                                                          \n",
      "                                 (64, None, 1024, 1                                               \n",
      "                                024, 1)]                                                          \n",
      "                                                                                                  \n",
      " tf.nn.softmax_4 (TFOpLambda)   (64, None, 1024, 10  0           ['tf.split_4[0][1]']             \n",
      "                                24, 1)                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (64, None, 1024, 10  0          ['tf.split_4[0][0]',             \n",
      " )                              24, 3)                            'tf.nn.softmax_4[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_4 (TFOpLamb  (64, 1024, 1024, 3)  0          ['tf.math.multiply_4[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,096,132\n",
      "Trainable params: 1,096,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# `image` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "# Convolutional encoder with position embedding.\n",
    "inputs = tf.keras.Input(shape=(1024,1024,1,))\n",
    "x = encoder_cnn(inputs)  # CNN Backbone.\n",
    "x = encoder_pos(x)  # Position embedding\n",
    "x = spatial_flatten(x)  # Flatten spatial dimensions (treat image as set).\n",
    "x = mlp(layer_norm(x))  # Feedforward network on set.\n",
    "# `x` has shape: [batch_size, width*height, input_size(64)].\n",
    "\n",
    "# Slot Attention module.\n",
    "slots = slot_attention(x)\n",
    "# `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "# Spatial broadcast decoder.\n",
    "x = spatial_broadcast(slots, decoder_initial_size)\n",
    "# `x` has shape: [batch_size*num_slots, width_init, height_init, slot_size].\n",
    "x = decoder_pos(x)\n",
    "x = decoder_cnn(x)\n",
    "# `x` has shape: [batch_size*num_slots, width, height, num_channels+1].\n",
    "\n",
    "# Undo combination of slot and batch dimension; split alpha masks.\n",
    "recons, masks = unstack_and_split(x, batch_size=64)\n",
    "# `recons` has shape: [batch_size, num_slots, width, height, num_channels].\n",
    "# `masks` has shape: [batch_size, num_slots, width, height, 1].\n",
    "\n",
    "# Normalize alpha masks over slots.\n",
    "masks = tf.nn.softmax(masks, axis=1)\n",
    "recon_combined = tf.reduce_sum(recons * masks, axis=1)  # Recombine image.\n",
    "# `recon_combined` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "outputs = recon_combined, recons, masks, slots\n",
    "\n",
    "slot_attention_ae = tf.keras.Model(inputs = inputs, outputs = outputs, name=\"Slot_Attention_AutoEnconder\")\n",
    "slot_attention_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5j9ZfBpeNa3",
    "outputId": "baae4701-4c3e-4700-b008-f2532dbdee0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training loop for object discovery with Slot Attention.\"\"\"\n",
    "\n",
    "# We use `tf.function` compilation to speed up execution. For debugging,\n",
    "# consider commenting out the `@tf.function` decorator.\n",
    "\n",
    "\n",
    "def l2_loss(prediction, target):\n",
    "  return tf.reduce_mean(tf.math.squared_difference(prediction, target))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch, model, optimizer):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  # Get the prediction of the models and compute the loss.\n",
    "  with tf.GradientTape() as tape:\n",
    "    preds = model(batch[\"image\"], training=True)\n",
    "    recon_combined, recons, masks, slots = preds\n",
    "    loss_value = l2_loss(recon_combined, batch[\"image\"])\n",
    "    del recons, masks, slots  # Unused.\n",
    "\n",
    "  # Get and apply gradients.\n",
    "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))   \n",
    "\n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses): \n",
    "    \"\"\"\n",
    "    Uses Matplotlib to visualize the losses of our model.\n",
    "    :param losses: list of loss data stored from train. Can use the model's loss_list \n",
    "    field \n",
    "\n",
    "    NOTE: DO NOT EDIT\n",
    "\n",
    "    :return: doesn't return anything, a plot should pop-up \n",
    "    \"\"\"\n",
    "    x = [i for i in range(len(losses))]\n",
    "    plt.plot(x, losses)\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model((128,128), 64, 7, 3, model_type=\"object_discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(64, 128, 128, 3)]       0         \n",
      "                                                                 \n",
      " slot_attention_auto_encoder  ((64, 128, 128, 3),      890308    \n",
      " _1 (SlotAttentionAutoEncode   (64, 7, 128, 128, 3),             \n",
      " r)                           (64, 7, 128, 128, 1),              \n",
      "                              (64, 7, 64))                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 890,308\n",
      "Trainable params: 890,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 15:47:29.353231: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-16 15:47:29.353342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 15:47:29.580398: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "Training Epochs:   0%|                                 | 0/5000 [00:00<?, ?it/s]2022-09-16 15:47:31.000186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "Training Epochs:   0%|                     | 24/5000 [03:07<11:04:06,  8.01s/it]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model.\n",
    "batch_size = 64\n",
    "num_slots = 7\n",
    "num_iterations = 3\n",
    "base_learning_rate = 0.0004\n",
    "num_train_steps = 5000\n",
    "warmup_steps = 5\n",
    "decay_rate = 0.5\n",
    "decay_steps = 100000\n",
    "tf.random.set_seed(0)\n",
    "resolution = (128, 128)\n",
    "\n",
    "# Build dataset iterators, optimizers and model.\n",
    "data_iterator = build_clevr_iterator(\n",
    "    batch_size, split=\"train\", resolution=resolution, shuffle=True,\n",
    "    max_n_objects=6, get_properties=False, apply_crop=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(base_learning_rate, epsilon=1e-08)\n",
    "\n",
    "model = build_model(resolution, batch_size, num_slots,\n",
    "                    num_iterations, model_type=\"object_discovery\")\n",
    "  \n",
    "# Prepare checkpoint manager.\n",
    "global_step = tf.Variable(\n",
    "    0, trainable=False, name=\"global_step\", dtype=tf.int64)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for _ in tqdm(range(num_train_steps), desc='Training Epochs'):\n",
    "    batch = next(data_iterator)\n",
    "\n",
    "    # Learning rate warm-up.\n",
    "    if global_step < warmup_steps:\n",
    "      learning_rate = base_learning_rate * tf.cast(\n",
    "          global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)\n",
    "    else:\n",
    "      learning_rate = base_learning_rate\n",
    "    \n",
    "    learning_rate = learning_rate * (decay_rate ** (\n",
    "        tf.cast(global_step, tf.float32) / tf.cast(decay_steps, tf.float32)))\n",
    "    optimizer.lr = learning_rate.numpy()\n",
    "\n",
    "    loss_value = train_step(batch, model, optimizer)\n",
    "    losses.append(loss_value)\n",
    "\n",
    "    # Update the global step. We update it before logging the loss and saving\n",
    "    # the model so that the last checkpoint is saved at the last iteration.\n",
    "    global_step.assign_add(1)\n",
    "    \n",
    "visualize_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1PWP4Q77v9k"
   },
   "outputs": [],
   "source": [
    "def renormalize(x):\n",
    "  \"\"\"Renormalize from [-1, 1] to [0, 1].\"\"\"\n",
    "  return x / 2. + 0.5\n",
    "\n",
    "def get_prediction(model, batch, idx=0):\n",
    "  recon_combined, recons, masks, slots = model(batch[\"image\"])\n",
    "  image = renormalize(batch[\"image\"])[idx]\n",
    "  recon_combined = renormalize(recon_combined)[idx]\n",
    "  recons = renormalize(recons)[idx]\n",
    "  masks = masks[idx]\n",
    "  return image, recon_combined, recons, masks, slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bhXod7Q7xXB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "resolution = (128,128)\n",
    "data_iterator = build_clevr_iterator(\n",
    "    batch_size, split=\"validation\", resolution=resolution, shuffle=True,\n",
    "    max_n_objects=6, get_properties=False, apply_crop=True)\n",
    "\n",
    "batch = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "plt.imshow(renormalize(batch[\"image\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzJouS6a7yvZ"
   },
   "outputs": [],
   "source": [
    "image, recon_combined, recons, masks, slots = get_prediction(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "num_slots = len(masks)\n",
    "fig, ax = plt.subplots(1, num_slots + 2, figsize=(15, 2))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(recon_combined)\n",
    "ax[1].set_title('Recon.')\n",
    "for i in range(num_slots):\n",
    "  ax[i + 2].imshow(recons[i] * masks[i] + (1 - masks[i]))\n",
    "  ax[i + 2].set_title('Slot %s' % str(i + 1))\n",
    "for i in range(len(ax)):\n",
    "  ax[i].grid(False)\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recon_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Slot Attention.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9 (Slot Attention)",
   "language": "python",
   "name": "slotattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
