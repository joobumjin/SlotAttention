{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iTGnEyGT7m0s"
   },
   "outputs": [],
   "source": [
    "#from absl import logging\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fTdux-_354b7"
   },
   "outputs": [],
   "source": [
    "\"\"\"Slot Attention model for object discovery and set prediction.\"\"\"\n",
    "\n",
    "class SlotAttention(layers.Layer):\n",
    "  \"\"\"Slot Attention module.\"\"\"\n",
    "\n",
    "  def __init__(self, num_iterations, num_slots, slot_size, mlp_hidden_size,\n",
    "               epsilon=1e-8):\n",
    "    \"\"\"Builds the Slot Attention module.\n",
    "    Args:\n",
    "      num_iterations: Number of iterations.\n",
    "      num_slots: Number of slots.\n",
    "      slot_size: Dimensionality of slot feature vectors.\n",
    "      mlp_hidden_size: Hidden layer size of MLP.\n",
    "      epsilon: Offset for attention coefficients before normalization.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.num_iterations = num_iterations\n",
    "    self.num_slots = num_slots\n",
    "    self.slot_size = slot_size\n",
    "    self.mlp_hidden_size = mlp_hidden_size\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    self.norm_inputs = layers.LayerNormalization()\n",
    "    self.norm_slots = layers.LayerNormalization()\n",
    "    self.norm_mlp = layers.LayerNormalization()\n",
    "\n",
    "    # Parameters for Gaussian init (shared by all slots).   # Intialize slots randomly at first \n",
    "    self.slots_mu = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],   # slot_size: Dimensionality of slot feature vectors.\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_mu\")\n",
    "    self.slots_log_sigma = self.add_weight(\n",
    "        initializer=\"glorot_uniform\",\n",
    "        shape=[1, 1, self.slot_size],\n",
    "        dtype=tf.float32,\n",
    "        name=\"slots_log_sigma\")\n",
    "\n",
    "    # Linear maps for the attention module.\n",
    "    self.project_q = layers.Dense(self.slot_size, use_bias=False, name=\"q\")\n",
    "    self.project_k = layers.Dense(self.slot_size, use_bias=False, name=\"k\")\n",
    "    self.project_v = layers.Dense(self.slot_size, use_bias=False, name=\"v\")\n",
    "\n",
    "    # Slot update functions.\n",
    "    self.gru = layers.GRUCell(self.slot_size)\n",
    "    self.mlp = tf.keras.Sequential([\n",
    "        layers.Dense(self.mlp_hidden_size, activation=\"relu\"),\n",
    "        layers.Dense(self.slot_size)\n",
    "    ], name=\"mlp\")\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # `inputs` has shape [batch_size, num_inputs, inputs_size].\n",
    "    inputs = self.norm_inputs(inputs)  # Apply layer norm to the input.\n",
    "    k = self.project_k(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create key vectors (based on inputs)\n",
    "    v = self.project_v(inputs)  # Shape: [batch_size, num_inputs, slot_size].  # create value vectors (based on inputs)\n",
    "\n",
    "    # Initialize the slots. Shape: [batch_size, num_slots, slot_size].\n",
    "    slots = self.slots_mu + tf.exp(self.slots_log_sigma) * tf.random.normal(\n",
    "        [tf.shape(inputs)[0], self.num_slots, self.slot_size])  # size: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    # Multiple rounds of attention.\n",
    "    for _ in range(self.num_iterations):\n",
    "      slots_prev = slots\n",
    "      slots = self.norm_slots(slots)\n",
    "\n",
    "      # Attention.\n",
    "      q = self.project_q(slots)  # Shape: [batch_size, num_slots, slot_size].  # create query vectors (based on slots)\n",
    "      q *= self.slot_size ** -0.5  # Normalization.\n",
    "      attn_logits = tf.keras.backend.batch_dot(k, q, axes=-1) # Batchwise dot product.\n",
    "      attn = tf.nn.softmax(attn_logits, axis=-1)\n",
    "      # `attn` has shape: [batch_size, num_inputs, num_slots]. \n",
    "      # attn represents how much attention each slot should pay to the features \n",
    "\n",
    "      # Weigted mean.\n",
    "      attn += self.epsilon\n",
    "      attn /= tf.reduce_sum(attn, axis=-2, keepdims=True) # summation; sum across the batch_size \n",
    "      updates = tf.keras.backend.batch_dot(attn, v, axes=-2)\n",
    "      # `updates` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "      # Slot update.\n",
    "      slots, _ = self.gru(updates, [slots_prev])   # output after gru has shape: [batch_size, num_slots, slot_size]\n",
    "      slots += self.mlp(self.norm_mlp(slots))      # # output after mlp has shape: [batch_size, num_slots, slot_size]\n",
    "\n",
    "    return slots\n",
    "\n",
    "\n",
    "def spatial_broadcast(slots, resolution):\n",
    "  \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n",
    "  # `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "  slots = tf.reshape(slots, [-1, slots.shape[-1]])[:, None, None, :]\n",
    "  grid = tf.tile(slots, [1, resolution[0], resolution[1], 1])   # this operation creates a new tensor by replicating input multiples times\n",
    "  # `grid` has shape: [batch_size*num_slots, width, height, slot_size].\n",
    "  return grid\n",
    "\n",
    "\n",
    "def spatial_flatten(x):\n",
    "  return tf.reshape(x, [-1, x.shape[1] * x.shape[2], x.shape[-1]])\n",
    "\n",
    "\n",
    "def unstack_and_split(x, batch_size, num_channels=3):\n",
    "  \"\"\"Unstack batch dimension and split into channels and alpha mask.\"\"\"\n",
    "  unstacked = tf.reshape(x, [batch_size, -1] + x.shape.as_list()[1:])\n",
    "  channels, masks = tf.split(unstacked, [num_channels, 1], axis=-1)\n",
    "  return channels, masks\n",
    "    \n",
    "\n",
    "def build_grid(resolution):\n",
    "  ranges = [np.linspace(0., 1., num=res) for res in resolution]\n",
    "  grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n",
    "  grid = np.stack(grid, axis=-1)\n",
    "  grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n",
    "  grid = np.expand_dims(grid, axis=0)\n",
    "  grid = grid.astype(np.float32)\n",
    "  return np.concatenate([grid, 1.0 - grid], axis=-1)\n",
    "\n",
    "\n",
    "class SoftPositionEmbed(layers.Layer):\n",
    "  \"\"\"Adds soft positional embedding with learnable projection.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size, resolution):\n",
    "    \"\"\"Builds the soft position embedding layer.\n",
    "    Args:\n",
    "      hidden_size: Size of input feature dimension.\n",
    "      resolution: Tuple of integers specifying width and height of grid.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.dense = layers.Dense(hidden_size, use_bias=True)\n",
    "    self.grid = build_grid(resolution)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.dense(self.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 17:34:41.555745: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-11 17:34:41.563314: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "resolution = (256,256)\n",
    "num_slots = 7\n",
    "num_iterations = 3\n",
    "\n",
    "encoder_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding=\"SAME\", activation=\"relu\")\n",
    "], name=\"encoder_cnn\")\n",
    "\n",
    "decoder_initial_size = (8, 8)\n",
    "decoder_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),  \n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 5, strides=(2, 2), padding=\"SAME\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(4, 3, strides=(1, 1), padding=\"SAME\", activation=None)\n",
    "], name=\"decoder_cnn\")\n",
    "\n",
    "encoder_pos = SoftPositionEmbed(64, resolution)\n",
    "decoder_pos = SoftPositionEmbed(64, decoder_initial_size)\n",
    "\n",
    "layer_norm = tf.keras.layers.LayerNormalization()\n",
    "mlp = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64)\n",
    "], name=\"encoded_feedforward\")\n",
    "\n",
    "slot_attention = SlotAttention(num_iterations=num_iterations, num_slots=num_slots, slot_size=64, mlp_hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Slot_Attention_AutoEnconder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " encoder_cnn (Sequential)       (None, 256, 256, 64  312256      ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " soft_position_embed (SoftPosit  (None, 256, 256, 64  320        ['encoder_cnn[0][0]']            \n",
      " ionEmbed)                      )                                                                 \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 65536, 64)    0           ['soft_position_embed[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 65536, 64)   128         ['tf.reshape[0][0]']             \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " encoded_feedforward (Sequentia  (None, 65536, 64)   8320        ['layer_normalization[0][0]']    \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " slot_attention (SlotAttention)  (None, 7, 64)       54336       ['encoded_feedforward[0][0]']    \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 64)           0           ['slot_attention[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1, 1, 64)    0           ['tf.reshape_1[0][0]']           \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (None, 8, 8, 64)     0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " soft_position_embed_1 (SoftPos  (None, 8, 8, 64)    320         ['tf.tile[0][0]']                \n",
      " itionEmbed)                                                                                      \n",
      "                                                                                                  \n",
      " decoder_cnn (Sequential)       (None, 256, 256, 4)  514628      ['soft_position_embed_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.reshape_2 (TFOpLambda)      (64, None, 256, 256  0           ['decoder_cnn[0][0]']            \n",
      "                                , 4)                                                              \n",
      "                                                                                                  \n",
      " tf.split (TFOpLambda)          [(64, None, 256, 25  0           ['tf.reshape_2[0][0]']           \n",
      "                                6, 3),                                                            \n",
      "                                 (64, None, 256, 25                                               \n",
      "                                6, 1)]                                                            \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)     (64, None, 256, 256  0           ['tf.split[0][1]']               \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (64, None, 256, 256  0           ['tf.split[0][0]',               \n",
      "                                , 3)                              'tf.nn.softmax[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (64, 256, 256, 3)   0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 890,308\n",
      "Trainable params: 890,308\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Convolutional encoder with position embedding.\n",
    "inputs = tf.keras.Input(shape=(256,256,3,))\n",
    "x = encoder_cnn(inputs)  # CNN Backbone.\n",
    "x = encoder_pos(x)  # Add positional embeddings to x\n",
    "x = spatial_flatten(x)  # Flatten spatial dimensions (treat image as set).\n",
    "x = mlp(layer_norm(x))  # Feedforward network on set.\n",
    "# `x` has shape: [batch_size, width*height, input_size(64)].\n",
    "\n",
    "# Slot Attention module.\n",
    "slots = slot_attention(x)\n",
    "# `slots` has shape: [batch_size, num_slots, slot_size].\n",
    "\n",
    "# Spatial broadcast decoder.\n",
    "x = spatial_broadcast(slots, decoder_initial_size)\n",
    "# `x` has shape: [batch_size*num_slots, width_init, height_init, slot_size].\n",
    "x = decoder_pos(x)\n",
    "x = decoder_cnn(x)\n",
    "# `x` has shape: [batch_size*num_slots, width, height, num_channels+1].\n",
    "\n",
    "# Undo combination of slot and batch dimension; split alpha masks.\n",
    "recons, masks = unstack_and_split(x, batch_size=64)\n",
    "# `recons` has shape: [batch_size, num_slots, width, height, num_channels].\n",
    "# `masks` has shape: [batch_size, num_slots, width, height, 1].\n",
    "\n",
    "# Normalize alpha masks over slots.\n",
    "masks = tf.nn.softmax(masks, axis=1)\n",
    "recon_combined = tf.reduce_sum(recons * masks, axis=1)  # Recombine image.\n",
    "# `recon_combined` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "outputs = recon_combined, recons, masks, slots\n",
    "\n",
    "slot_attention_ae = tf.keras.Model(inputs = inputs, outputs = outputs, name=\"Slot_Attention_AutoEnconder\")\n",
    "slot_attention_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import tifffile\n",
    "import quilt3 as q3\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from aicsimageio import AICSImage #=> this package was really difficult to install, maybe using an automated yaml would be good\n",
    "from PIL import Image\n",
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import math\n",
    "\n",
    "def fetch_data():\n",
    "    package = q3.Package.browse(\n",
    "        \"aics/pipeline_integrated_single_cell\",\n",
    "        registry=\"s3://allencell\"\n",
    "    )\n",
    "    \n",
    "    package[\"cell_images_2d\"].fetch(\"./AllenCell/cell_images_2d/\")\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "def allen_cell_dataset(download_data = False, batch_size = 64):\n",
    "    # 70 train: 35k \n",
    "    # 20 validation: 10k\n",
    "    # 10 test: 5k\n",
    "    if download_data:\n",
    "        fetch_data()\n",
    "        \n",
    "    def convert_to_padded_tensor(img):\n",
    "        image_tensor = tf.convert_to_tensor(img.data[0][0], dtype=tf.float32)\n",
    "        padded_tensor = tf.image.resize_with_crop_or_pad(image_tensor, 256, 256)\n",
    "        return padded_tensor\n",
    "    \n",
    "    \n",
    "    imgs = []\n",
    "    file_names = [join(\"./AllenCell/cell_images_2d/\", f) for f in listdir(\"./AllenCell/cell_images_2d/\") if join(\"./AllenCell/cell_images_2d/\", f).endswith(\".png\")]\n",
    "    \n",
    "    if len(file_names) == 0:\n",
    "        raise Exception(\"No .png Files in the AllenCell directory.\")\n",
    "        \n",
    "    for ind, file_name in enumerate(tqdm(file_names, desc=\"loading data\")):\n",
    "        #if ind < 10000:\n",
    "        img = AICSImage(file_name)\n",
    "        tensor = convert_to_padded_tensor(img)\n",
    "        imgs.append(tensor[0])\n",
    "        \n",
    "    print(f\"num images: {len(imgs)}\")\n",
    "        \n",
    "    ## split into train validation test\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "    \n",
    "    num_sample = len(dataset)\n",
    "    print(f\"length of dataset: {len(dataset)}\")\n",
    "    dataset = dataset.shuffle(buffer_size = len(dataset))\n",
    "    \n",
    "    num_train = math.ceil(num_sample * 0.7)\n",
    "    print(f\"num train: {num_train}\")\n",
    "    num_val = math.floor(num_sample * 0.2)\n",
    "    print(f\"num val: {num_val}\")\n",
    "    num_test = math.floor(num_sample * 0.1)\n",
    "    print(f\"num test: {num_test}\")\n",
    "    \n",
    "    train = dataset.take(num_train)\n",
    "    print(f\"train unbatched: {len(train)}\")\n",
    "    train = train.batch(batch_size)\n",
    "    print(f\" batched: {len(train)}\")\n",
    "    test_val = dataset.skip(num_train)\n",
    "    \n",
    "    test = test_val.take(num_test)\n",
    "    print(f\"test unbatched: {len(test)}\")\n",
    "    test = test.batch(batch_size)\n",
    "    print(f\" batched: {len(test)}\")\n",
    "    \n",
    "    val = test_val.skip(num_test)\n",
    "    print(f\"val unbatched: {len(val)}\")\n",
    "    val = val.batch(batch_size)\n",
    "    print(f\" batched: {len(val)}\")\n",
    "\n",
    "    return train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_data()\n",
    "#do not run this again, all data has been fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5j9ZfBpeNa3",
    "outputId": "baae4701-4c3e-4700-b008-f2532dbdee0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training loop for object discovery with Slot Attention.\"\"\"\n",
    "\n",
    "# We use `tf.function` compilation to speed up execution. For debugging,\n",
    "# consider commenting out the `@tf.function` decorator.\n",
    "\n",
    "\n",
    "def l2_loss(prediction, target):\n",
    "  return tf.reduce_mean(tf.math.squared_difference(prediction, target))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch, model, optimizer):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  # Get the prediction of the models and compute the loss.\n",
    "  with tf.GradientTape() as tape:\n",
    "    preds = model(batch, training=True)\n",
    "    recon_combined, recons, masks, slots = preds\n",
    "    loss_value = l2_loss(recon_combined, batch)\n",
    "    del recons, masks, slots  # Unused.\n",
    "\n",
    "  # Get and apply gradients.\n",
    "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))   \n",
    "\n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_step(next(train_iterator), slot_attention_ae, tf.keras.optimizers.Adam(base_learning_rate, epsilon=1e-08))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses): \n",
    "    \"\"\"\n",
    "    Uses Matplotlib to visualize the losses of our model.\n",
    "    :param losses: list of loss data stored from train. Can use the model's loss_list \n",
    "    field \n",
    "\n",
    "    NOTE: DO NOT EDIT\n",
    "\n",
    "    :return: doesn't return anything, a plot should pop-up \n",
    "    \"\"\"\n",
    "    x = [i for i in range(len(losses))]\n",
    "    plt.plot(x, losses)\n",
    "    plt.title('Loss per epoch')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataset iterators\n",
    "train_iterator, test_iterator, val_iterator = allen_cell_dataset(False, batch_size)\n",
    "train_iterator = cycle(list(train_iterator))\n",
    "test_iterator = cycle(list(test_iterator))\n",
    "val_iterator = cycle(list(val_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data: 100%|███████████████████████████| 49325/49325 [06:48<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num images: 49325\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model.\n",
    "batch_size = 64\n",
    "num_slots = 7\n",
    "num_iterations = 3\n",
    "base_learning_rate = 0.0004\n",
    "num_train_steps = 500\n",
    "warmup_steps = 5\n",
    "decay_rate = 0.5\n",
    "decay_steps = 100000\n",
    "#tf.random.set_seed(0)\n",
    "resolution = (256, 256)\n",
    "\n",
    "#checkpoint_path = \"./training/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "# Build optimizers and model\n",
    "optimizer = tf.keras.optimizers.Adam(base_learning_rate, epsilon=1e-08)\n",
    "\n",
    "#model = build_model(resolution, batch_size, num_slots, num_iterations, model_type=\"object_discovery\")\n",
    "\n",
    "# Prepare checkpoint manager.\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\", dtype=tf.int64)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for _ in tqdm(range(num_train_steps), desc='Training Epochs'):\n",
    "    batch = next(train_iterator)\n",
    "    val_batch = next(val_iterator)\n",
    "\n",
    "    # Learning rate warm-up.\n",
    "    if global_step < warmup_steps:\n",
    "        learning_rate = base_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)\n",
    "    else:\n",
    "        learning_rate = base_learning_rate\n",
    "\n",
    "    learning_rate = learning_rate * (decay_rate ** (tf.cast(global_step, tf.float32) / tf.cast(decay_steps, tf.float32)))\n",
    "    optimizer.lr = learning_rate.numpy()\n",
    "\n",
    "    loss_value = train_step(batch, slot_attention_ae, optimizer)\n",
    "    losses.append(loss_value)\n",
    "    \n",
    "    val_losses.append(slot_attention_ae(batch, training=False))\n",
    "\n",
    "    # Update the global step. We update it before logging the loss and saving\n",
    "    # the model so that the last checkpoint is saved at the last iteration.\n",
    "    global_step.assign_add(1)\n",
    "\n",
    "visualize_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1PWP4Q77v9k"
   },
   "outputs": [],
   "source": [
    "def renormalize(x):\n",
    "  \"\"\"Renormalize from [-1, 1] to [0, 1].\"\"\"\n",
    "  return x / 2. + 0.5\n",
    "\n",
    "def get_prediction(model, batch, idx=0):\n",
    "  recon_combined, recons, masks, slots = model(batch)\n",
    "  image = renormalize(batch)[idx]\n",
    "  recon_combined = renormalize(recon_combined)[idx]\n",
    "  recons = renormalize(recons)[idx]\n",
    "  masks = masks[idx]\n",
    "  return image, recon_combined, recons, masks, slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bhXod7Q7xXB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "resolution = (256,256)\n",
    "\n",
    "batch = next(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "plt.imshow(renormalize(batch)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzJouS6a7yvZ"
   },
   "outputs": [],
   "source": [
    "image, recon_combined, recons, masks, slots = get_prediction(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize.\n",
    "num_slots = len(masks)\n",
    "fig, ax = plt.subplots(1, num_slots + 2, figsize=(15, 2))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(recon_combined)\n",
    "ax[1].set_title('Recon.')\n",
    "for i in range(num_slots):\n",
    "  ax[i + 2].imshow(recons[i] * masks[i] + (1 - masks[i]))\n",
    "  ax[i + 2].set_title('Slot %s' % str(i + 1))\n",
    "for i in range(len(ax)):\n",
    "  ax[i].grid(False)\n",
    "  ax[i].axis('off')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Slot Attention.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9 (Slot Attention)",
   "language": "python",
   "name": "slotattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
